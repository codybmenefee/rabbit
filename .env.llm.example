# =============================================================================
# LLM-Enhanced YouTube Scraping Configuration
# =============================================================================
# Copy this file to .env and configure the LLM scraping service
# This enables AI-powered data extraction that's more resilient to YouTube changes

# =============================================================================
# LLM Service Configuration
# =============================================================================

# Enable LLM scraping service
LLM_SCRAPING_ENABLED=true

# LLM Provider: 'anthropic' or 'openai'
# Anthropic Claude is recommended for cost-effectiveness
LLM_PROVIDER=anthropic

# API Keys (provide one based on your chosen provider)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# Model Configuration
# =============================================================================

# Model selection (provider-specific)
# Anthropic: claude-3-haiku-20240307 (recommended), claude-3-sonnet-20240229
# OpenAI: gpt-3.5-turbo (recommended), gpt-4-turbo-preview
LLM_MODEL=claude-3-haiku-20240307

# Token limits and temperature
LLM_MAX_TOKENS=2000
LLM_TEMPERATURE=0.1

# =============================================================================
# Performance & Cost Controls
# =============================================================================

# Concurrent requests (keep low to avoid rate limits)
LLM_MAX_CONCURRENT_REQUESTS=5

# Request timing (milliseconds)
LLM_REQUEST_DELAY_MS=1000
LLM_TIMEOUT_MS=30000
LLM_RETRY_ATTEMPTS=2

# Batch processing
LLM_BATCH_SIZE=10

# Cost management (USD)
LLM_COST_LIMIT=10.0

# =============================================================================
# Caching & Performance
# =============================================================================

# Enable caching to reduce costs
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=7200

# HTML processing
LLM_HTML_CHUNK_SIZE=50000

# Network configuration
LLM_CONNECTION_POOL_SIZE=20

# Fallback options
LLM_ENABLE_FALLBACK=true

# =============================================================================
# Usage Examples
# =============================================================================

# Cost Estimation for 18,154 videos:
# - Claude Haiku: ~$57 total (~$0.003 per video)
# - GPT-3.5-turbo: ~$114 total (~$0.006 per video)

# Recommended batch processing:
# - Process 1,000 videos per day with $10 daily budget
# - Use LLM_BATCH_SIZE=10 for optimal performance
# - Monitor costs with /api/llm-scraping/metrics endpoint

# =============================================================================
# Additional Backend Configuration
# =============================================================================

# These should already be in your main .env file
# PORT=5000
# NODE_ENV=development
# MONGODB_URI=mongodb://localhost:27017/rabbit-analytics

# Rate limiting for LLM endpoints
# RATE_LIMIT_WINDOW_MS=900000
# RATE_LIMIT_MAX_REQUESTS=100