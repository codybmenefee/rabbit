# ============================================================================
# Rabbit YouTube Analytics Platform - Environment Configuration Example
# ============================================================================
# Copy this file to .env and configure with your actual values
# 
# This file contains all environment variables used across the platform:
# - Backend API server configuration
# - Frontend Next.js application configuration
# - LLM and AI service integration
# - Database and external service connections
# - Logging and monitoring setup

# ============================================================================
# Application Configuration
# ============================================================================

# Server Configuration
PORT=5000
NODE_ENV=development

# ============================================================================
# Database Configuration
# ============================================================================

# MongoDB connection string
# For local development: mongodb://localhost:27017/youtube-analytics
# For production: mongodb://username:password@host:port/database
MONGODB_URI=mongodb://localhost:27017/youtube-analytics

# ============================================================================
# CORS & Security Configuration
# ============================================================================

# Allowed origins for CORS (comma-separated)
CORS_ORIGIN=http://localhost:3000,http://localhost:3001

# Rate Limiting Configuration
RATE_LIMIT_WINDOW_MS=900000          # 15 minutes in milliseconds
RATE_LIMIT_MAX_REQUESTS=100          # Max requests per window per IP

# File Upload Configuration
MAX_FILE_SIZE=50mb                   # Maximum file upload size
UPLOAD_TIMEOUT=300000                # Upload timeout in milliseconds (5 minutes)

# ============================================================================
# YouTube API Configuration
# ============================================================================

# YouTube Data API v3 key (get from Google Cloud Console)
YOUTUBE_API_KEY=your_youtube_api_key_here

# API Usage Limits
YOUTUBE_QUOTA_LIMIT=10000            # Daily quota limit
BATCH_SIZE=50                        # Batch size for API requests
API_DELAY_MS=100                     # Delay between API requests (ms)
MAX_CONCURRENT_REQUESTS=5            # Max concurrent API requests

# ============================================================================
# Web Scraping Configuration
# ============================================================================

# Enable/disable web scraping as fallback to YouTube API
SCRAPING_ENABLED=true

# Scraping Performance Settings
SCRAPING_CONCURRENT_REQUESTS=3       # Max concurrent scraping requests
SCRAPING_DELAY_MS=2000              # Delay between scraping requests (ms)
SCRAPING_TIMEOUT_MS=30000           # Request timeout (ms)
SCRAPING_RETRY_ATTEMPTS=3           # Number of retry attempts

# Scraping Features
SCRAPING_ENABLE_JAVASCRIPT=false    # Enable JavaScript rendering (slower)
SCRAPING_ENABLE_BROWSER=true        # Use browser-like headers
SCRAPING_CACHE_ENABLED=true         # Enable result caching
SCRAPING_CACHE_TTL=86400            # Cache TTL in seconds (24 hours)

# Service Selection
# Options: "api" (YouTube API only), "scraping" (scraping only), "auto" (API with scraping fallback)
DEFAULT_ENRICHMENT_SERVICE=auto

# ============================================================================
# LLM Configuration (AI-Powered Scraping)
# ============================================================================

# Enable LLM-enhanced scraping for better data extraction
LLM_SCRAPING_ENABLED=true

# LLM Provider Selection
# Options: 'anthropic', 'openai', 'meta', 'google', 'mistral'
LLM_PROVIDER=google

# Model Configuration
# Google: gemma-3-4b-it (recommended), gemma-3n-e4b-it, gemini-pro
# Anthropic: claude-3-haiku-20240307, claude-3-sonnet-20240229
# OpenAI: gpt-3.5-turbo, gpt-4-turbo-preview, gpt-4o
# Meta: llama-3.1-8b-instruct, llama-3.1-70b-instruct
# Mistral: mistral-7b-instruct, mixtral-8x7b-instruct
LLM_MODEL=gemma-3-4b-it

# OpenRouter API Configuration (required for most models)
# Get API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_REFERER=https://your-site.com
OPENROUTER_TITLE=Your App Name

# LLM Performance Settings
LLM_MAX_TOKENS=2000                  # Max tokens per request
LLM_TEMPERATURE=0.1                  # Temperature (0.0-1.0, lower = more deterministic)
LLM_MAX_CONCURRENT_REQUESTS=5        # Max concurrent LLM requests
LLM_REQUEST_DELAY_MS=1000           # Delay between LLM requests (ms)
LLM_RETRY_ATTEMPTS=2                # Number of retry attempts
LLM_TIMEOUT_MS=30000                # Request timeout (ms)
LLM_BATCH_SIZE=10                   # Batch size for processing
LLM_COST_LIMIT=10.0                 # Maximum cost per session ($USD)

# Caching and Optimization
LLM_CACHE_ENABLED=true              # Enable result caching
LLM_CACHE_TTL=7200                  # Cache TTL in seconds (2 hours)
LLM_HTML_CHUNK_SIZE=50000           # HTML chunk size for processing
LLM_ENABLE_FALLBACK=true            # Fallback to traditional scraping on failure

# High Performance Scraping (Optional - for large datasets)
HP_SCRAPING_ENABLED=false           # Enable high-performance mode
HP_SCRAPING_MAX_CONCURRENT_REQUESTS=500
HP_SCRAPING_REQUEST_DELAY_MS=100
HP_SCRAPING_RETRY_ATTEMPTS=2
HP_SCRAPING_TIMEOUT_MS=15000
HP_SCRAPING_ENABLE_WORKER_THREADS=false
HP_SCRAPING_WORKER_THREAD_COUNT=16
HP_SCRAPING_CONNECTION_POOL_SIZE=50
HP_SCRAPING_BATCH_SIZE=100
HP_SCRAPING_ENABLE_DEDUPLICATION=true
HP_SCRAPING_CACHE_ENABLED=true
HP_SCRAPING_CACHE_TTL=3600
HP_SCRAPING_ENABLE_FAST_PARSING=true
HP_SCRAPING_MAX_MEMORY_USAGE=2048

# ============================================================================
# Logging Configuration
# ============================================================================

# Basic Logging Settings
LOG_LEVEL=debug                     # Log level: error, warn, info, debug
LOG_FORMAT=json                     # Log format: json or simple
LOG_TO_FILE=true                    # Enable file logging
LOG_MAX_FILE_SIZE=10MB              # Max log file size
LOG_MAX_FILES=5                     # Max number of log files
LOG_DATE_PATTERN=YYYY-MM-DD         # Log file date pattern

# Advanced Logging Features
ENABLE_REQUEST_LOGGING=true         # Log HTTP requests
ENABLE_PERFORMANCE_MONITORING=true  # Enable performance monitoring
LOG_SLOW_QUERIES=true              # Log slow database queries
SLOW_QUERY_THRESHOLD=1000          # Slow query threshold (ms)
ENABLE_CORRELATION_IDS=true        # Enable request correlation IDs
LOG_SANITIZE_SENSITIVE=true        # Sanitize sensitive data in logs

# Development Logging Features
ENABLE_DETAILED_LOGGING=true       # Enable detailed logging
LOG_API_REQUESTS=true              # Log API requests
LOG_DATABASE_OPERATIONS=true       # Log database operations
LOG_EXTERNAL_SERVICE_CALLS=true    # Log external service calls

# ============================================================================
# Frontend Configuration (Next.js)
# ============================================================================

# API Configuration
NEXT_PUBLIC_API_URL=http://localhost:5000  # Backend API URL

# Application Configuration
NEXT_PUBLIC_APP_NAME="Rabbit Analytics"
NEXT_PUBLIC_APP_VERSION="2.0.0"

# Feature Flags
NEXT_PUBLIC_ENABLE_API_ENRICHMENT=true    # Enable API enrichment features
NEXT_PUBLIC_ENABLE_EXPORT=true            # Enable export functionality
NEXT_PUBLIC_ENABLE_DARK_MODE=true         # Enable dark mode toggle

# Frontend Logging
NEXT_PUBLIC_LOG_LEVEL=info                # Frontend log level
NEXT_PUBLIC_ENABLE_CONSOLE_LOGGING=true   # Enable console logging
NEXT_PUBLIC_ENABLE_REMOTE_LOGGING=false   # Enable remote logging
NEXT_PUBLIC_LOG_API_CALLS=true            # Log API calls
NEXT_PUBLIC_LOG_USER_INTERACTIONS=false   # Log user interactions
NEXT_PUBLIC_LOG_PERFORMANCE=true          # Log performance metrics
NEXT_PUBLIC_ENABLE_ERROR_BOUNDARY_LOGGING=true  # Enable error boundary logging

# Development Settings
NEXT_PUBLIC_DEBUG_MODE=false              # Enable debug mode
NEXT_PUBLIC_ENABLE_DEBUG_LOGGING=false    # Enable debug logging

# ============================================================================
# Optional Services (Monitoring & Analytics)
# ============================================================================

# Sentry Error Tracking
SENTRY_DSN=                               # Sentry DSN for error tracking
NEXT_PUBLIC_SENTRY_DSN=                   # Sentry DSN for frontend
ENABLE_SENTRY_LOGGING=false               # Enable Sentry integration

# DataDog Monitoring
DATADOG_API_KEY=                          # DataDog API key
DATADOG_CLIENT_TOKEN=                     # DataDog client token
DATADOG_APPLICATION_ID=                   # DataDog application ID
ENABLE_DATADOG_LOGGING=false              # Enable DataDog integration

# LogTail Logging Service
LOGTAIL_SOURCE_TOKEN=                     # LogTail source token
NEXT_PUBLIC_LOGTAIL_SOURCE_TOKEN=         # LogTail token for frontend
ENABLE_LOGTAIL_LOGGING=false              # Enable LogTail integration

# Google Analytics
NEXT_PUBLIC_GA_TRACKING_ID=               # Google Analytics tracking ID

# Remote Logging Endpoint (Custom)
NEXT_PUBLIC_REMOTE_LOG_ENDPOINT=          # Custom remote logging endpoint
NEXT_PUBLIC_LOG_API_KEY=                  # API key for remote logging

# ============================================================================
# Cost Estimates for LLM Services (as of 2024)
# ============================================================================
# 
# Google Gemma 3 4B Instruct (via OpenRouter):
#   - Input: ~$0.00000002 per token
#   - Output: ~$0.00000004 per token
#   - Estimated cost per video: ~$0.000006
#   - Very cost-effective for large datasets
#
# Anthropic Claude 3 Haiku:
#   - Input: $0.25 per 1M tokens
#   - Output: $1.25 per 1M tokens
#   - Estimated cost per video: ~$0.0015
#   - Good balance of quality and cost
#
# OpenAI GPT-3.5 Turbo:
#   - Input: $1.5 per 1M tokens
#   - Output: $2 per 1M tokens
#   - Estimated cost per video: ~$0.003
#   - Reliable performance
#
# Meta Llama 3.1 8B:
#   - Input/Output: $0.2 per 1M tokens
#   - Estimated cost per video: ~$0.0004
#   - Open source, cost-effective
#
# For 18,000+ videos, estimated total costs:
# - Gemma 3 4B: ~$0.11
# - Claude 3 Haiku: ~$27
# - GPT-3.5 Turbo: ~$54
# - Llama 3.1 8B: ~$7.20